{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation of modules\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from skimage import filters\n",
    "from time import perf_counter\n",
    "import os\n",
    "from cropping import masking\n",
    "from pprint import pprint\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Dictionary and sets created\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary location:value --> {'270-01-01': 's_2-s_7-s_0-s_pt',...}\n",
    "\n",
    "def get_transcription(transcription_file):\n",
    "    with open(transcription_file, 'r') as f:\n",
    "        names = {}\n",
    "        for line in f.readlines():\n",
    "            names[line.split(\" \")[0]] = line.split(\" \")[1].strip()\n",
    "    return names\n",
    "\n",
    "def get_list_of_text(file):\n",
    "    with open(file, 'r') as f:\n",
    "        nameOfText = []\n",
    "        for line in f.readlines():\n",
    "            nameOfText.append(line.strip())\n",
    "    return nameOfText\n",
    "\n",
    "transciptionOfAllWords = get_transcription(\"./../ground-truth/transcription.txt\")\n",
    "trainingFiles = get_list_of_text(\"./../task/train.txt\")\n",
    "validationFiles = get_list_of_text(\"./../task/valid.txt\")\n",
    "print(\">> Dictionary and sets created\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Cropping picture 270  ...\n",
      " Picture 270 cropped in: 198s\n",
      ">> Cropping picture 271  ...\n",
      " Picture 271 cropped in: 279s\n",
      ">> Cropping picture 272  ...\n",
      " Picture 272 cropped in: 235s\n",
      ">> Cropping picture 273  ...\n",
      " Picture 273 cropped in: 252s\n",
      ">> Cropping picture 274  ...\n",
      " Picture 274 cropped in: 316s\n",
      ">> Cropping picture 275  ...\n",
      " Picture 275 cropped in: 313s\n",
      ">> Cropping picture 276  ...\n",
      " Picture 276 cropped in: 220s\n",
      ">> Cropping picture 277  ...\n",
      " Picture 277 cropped in: 270s\n",
      ">> Cropping picture 278  ...\n",
      " Picture 278 cropped in: 177s\n",
      ">> Cropping picture 279  ...\n",
      " Picture 279 cropped in: 292s\n",
      ">> Cropping picture 300  ...\n",
      " Picture 300 cropped in: 183s\n",
      ">> Cropping picture 301  ...\n",
      " Picture 301 cropped in: 330s\n",
      ">> Cropping picture 302  ...\n",
      " Picture 302 cropped in: 336s\n",
      ">> Cropping picture 303  ...\n",
      " Picture 303 cropped in: 329s\n",
      ">> Cropping picture 304  ...\n",
      " Picture 304 cropped in: 219s\n"
     ]
    }
   ],
   "source": [
    "# Just do that one time to generate the .pkl files -> If files are already available skip this cell\n",
    "def cropPictures():\n",
    "    allPictures = os.listdir(\"./../images\")\n",
    "\n",
    "    if not \"pkls\" in os.listdir(\".\"):\n",
    "        os.makedirs(\"./pkls\")\n",
    "\n",
    "    for picture in allPictures:\n",
    "    \n",
    "        print(\">> Cropping picture\", picture[:-4], \" ...\")\n",
    "        start_time = time.perf_counter()\n",
    "        numberOfPicture = picture[:-4]\n",
    "\n",
    "        # Get all split words\n",
    "        trimmed = masking(int(numberOfPicture))\n",
    "    \n",
    "        # Serialize the objects with pickle --> creation of all .pkl files\n",
    "        with open(os.path.join(\"./pkls\", numberOfPicture), 'wb') as f:\n",
    "            pickle.dump(trimmed, f, pickle.HIGHEST_PROTOCOL)\n",
    "        print(\" Picture %s cropped in: %ss\" % (picture[:-4], int(time.perf_counter()-start_time)))\n",
    "        \n",
    "cropPictures()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarizeAndStoreInDict(typeOfSet):\n",
    "    allPkls = os.listdir(\"./pkls\")\n",
    "\n",
    "    #if not \"cropped\" in os.listdir(\".\"):\n",
    "        #os.makedirs(\"./cropped\")\n",
    "\n",
    "    # Dictionary containing the identifier and the values of all pixels of each word\n",
    "    allWordsAsNumpyMatrix={}\n",
    "    height=0\n",
    "    width=0\n",
    "    \n",
    "    # Get maximum width and maximum height of all words in order to create a picture of same size \n",
    "    # for all words of all pages\n",
    "    for text in allPkls:\n",
    "        with open('./pkls/'+text,'rb') as f:\n",
    "            allWords = pickle.load(f)\n",
    "            maxHeightPage = max([x.shape[0] for x in allWords.values()])\n",
    "            maxWidthPage = max([x.shape[1] for x in allWords.values()])\n",
    "            if maxHeightPage>height:\n",
    "                height= maxHeightPage\n",
    "            if maxWidthPage > width:\n",
    "                width= maxWidthPage\n",
    "\n",
    "    for text in allPkls:\n",
    "        if text not in typeOfSet:\n",
    "            continue\n",
    "        with open('./pkls/'+text,'rb') as f:\n",
    "            allWords = pickle.load(f)\n",
    "            \n",
    "            for wordpkl in allWords:\n",
    "                word = allWords[wordpkl].filled(255)\n",
    "            \n",
    "                # Only keep the dark parts\n",
    "                tresh = filters.threshold_otsu(word)\n",
    "                foo = word<tresh\n",
    "            \n",
    "                # Set every \"word\" as a matrix of same size.\n",
    "                target = np.zeros((height,width))\n",
    "                target[:foo.shape[0],:foo.shape[1]]=foo\n",
    "            \n",
    "                # return a dictionary '304-01-01':[[0,0,0,0,0,0,1,...,0,..], [], [0,1,0,0,..], []..]\n",
    "                allWordsAsNumpyMatrix[wordpkl]=target\n",
    "                \n",
    "    return allWordsAsNumpyMatrix\n",
    "allWords_training = binarizeAndStoreInDict(trainingFiles)\n",
    "allWords_validation = binarizeAndStoreInDict(validationFiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# JUST A TEST TO SEE IF ARRAYS OF FEATURES ARE WELL CREATED\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%reload_ext autoreload\n",
    "# Transform each picture of word into an array of features and store it in a dictionary identifier: allfeatures\n",
    "# --> {'304-01-01': [feature1, feature2, feature3, feature4, feature5]}\n",
    "from features import *\n",
    "allWords_training_features = calculate_features_of_all_samples(allWords_training)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13871.362575372024\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "#Just a test to compare the distance of an image with itself (should equals 0) and an image with another one\n",
    "from dtw import *\n",
    "\n",
    "score1 = dtw(allWords_training_features['270-09-06'],allWords_training_features['270-05-07'])\n",
    "score2 = dtw(allWords_training_features['270-09-06'],allWords_training_features['270-09-06'])\n",
    "print(score1)\n",
    "print(score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IS TOO SLOW FOR NOW... NOT TESTED\n",
    "import operator\n",
    "\n",
    "def kNN_DTW(allWords_training, allWords_test, k, transcriptionOfAllWords):\n",
    "# Returns the k nearest neighbors through DTW distance algorithm\n",
    "    allWords_training_features = calculate_features_of_all_samples(allWords_training)\n",
    "    allWords_test_features = calculate_features_of_all_samples(allWords_test)\n",
    "    \n",
    "    for test_image in allWords_test_features.keys():\n",
    "        # compute distance between each training instance and the test instance\n",
    "        # and store tuple of result and corresponding training instance\n",
    "        distances = []\n",
    "        for training_image in allWords_training_features.keys():\n",
    "            dist = dtw(allWords_test_features[test_image], allWords_training_features[training_image])\n",
    "            distances.append((transciptionOfAllWords[training_image], dist, training_image))\n",
    "        # sort list of tuples of distances in ascending order \n",
    "        # regarding the second item of the tuples, i.e. the distances\n",
    "        distances.sort(key=operator.itemgetter(1))\n",
    "        kNNforMajority = []\n",
    "        locationOfWords = []\n",
    "        for x in range(k):\n",
    "            kNN.append(distances[x][0])\n",
    "            locationOfWords.append(distances[x][2])\n",
    "        # return the k first training instances (the k most similar)\n",
    "        return majority(kNN), locationOfWords\n",
    "\n",
    "\n",
    "def majority(kNN):\n",
    "# Returns the label occuring the most among the k nearest neighbors\n",
    "    # classVotes is a key:value list\n",
    "    classVotes = {}\n",
    "    for x in range(len(kNN)):\n",
    "        # extract the label of training instance \n",
    "        response = kNN[x][0]\n",
    "        # if the label appears in list of classVotes \n",
    "        if response in classVotes:\n",
    "        # add vote at to label key\n",
    "            classVotes[response] += 1  \n",
    "        else:\n",
    "            classVotes[response] = 1\n",
    "    # sort the classVotes according to the number of votes in a descending order\n",
    "    sortedVotes = sorted(classVotes.iteritems(), key=operator.itemgetter(1), reverse=True)\n",
    "    return sortedVotes[0][0]\n",
    "\n",
    "# test with one word\n",
    "label, locations = kNN_DTW(allWords_training, {'304-32-03':allWords_validation['304-32-03']},10, transciptionOfAllWords)\n",
    "print(\"The words that are found are\", label)\n",
    "print(\"They were found at \", locations)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
